Some notes and reminders to myself about TF:

It uses symbolic computations. Don't assume normal Python variables. The purpose
is to use potentially complicated expressions and math formulas and encode it
into a "dataflow" graph, which is then offloaded to a Tensorflow "Session" which
can efficiently execute the entire computation. For instance, with
backpropagation, it uses symbolic differentiation. I hope it's basically a C++
version of what we did in CS 294-129 at Berkeley. Also, make sure you build the
graph expressions before actually starting a session (as a general rule).

Feeding: use feed_dict as input to Sessions so that Tensors can be turned into
numpy arrays. Closely related to these are tf.placeholder()s, which _must_ be
fed with some data (these are the Tensors btw). Recall that the feed_dict has
tf.placeholder()s as dictionary arguments. Use the shape argument for
placeholders to keep track of the data size and to catch bugs.

Convolutions: make sure to read the API docs carefully to get dimensions to line
up. In particular, the conv2d expects that one dimension is the number of input
channels, and another is the number of output channels, so if it's 32 and 64,
then the input was perhaps a (x,x,32)-dimensional tensor and the output would be
(x,x,64)-dimensional, assuming that the padding is set to 'SAME' to avoid
confusing resizes. The x in MNIST is 28, of course.

Tensorboard -- hopefully I can use this. I'll update these notes later.

Custom format data -- see the FAQ. Hopefully this will work and I can update
these notes. TODO try to redo MNIST but starting from custom python data and
_not_ using their built-in methods.
