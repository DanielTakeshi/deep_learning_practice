Some notes and reminders to myself about TF:

It uses symbolic computations. Don't assume normal Python variables. The purpose
is to use potentially complicated expressions and math formulas and encode it
into a "dataflow" graph, which is then offloaded to a Tensorflow "Session" which
can efficiently execute the entire computation. For instance, with
backpropagation, it uses symbolic differentiation. I hope it's basically a C++
version of what we did in CS 294-129 at Berkeley. Also, make sure you build the
graph expressions before actually starting a session (as a general rule).
Sessions have to be initialized and run appropriately. Check the tutorials.

Feeding: use feed_dict as input to Sessions so that Tensors can be turned into
numpy arrays. Closely related to these are tf.placeholder()s, which _must_ be
fed with some data (these are the Tensors btw). Recall that the feed_dict has
tf.placeholder()s as dictionary arguments. Use the shape argument for
placeholders to keep track of the data size and to catch bugs.

sess = tf.Session() vs sess = tf.InteractiveSession(), looks like I should use
the second one because that's less typing, I don't need to do sess.run(...) but
can do (...).eval() and I think (...).run(), and I use (...).run() in the first
two tutorials. But InteractiveSession() was apparently designed for IPython and
Jupyter Notebooks, but I'm not seeing the advantage of Session() over
InteractiveSession(). I'll have to revisit this out later. EDIT: maybe
sess.run() is better to keep things standardized. Also, sess.run() can take in
a list of arguments so that it returns values corresponding to each argument. It
will run the necessary graph fragments to execute each operation and evaluate
each tensor. Note: the list which is inputted to sess.run() should be a list of
_operations_ and _tensors_ or closely related types. See:
https://www.tensorflow.org/api_docs/python/client/session_management#Session.run

Convolutions: make sure to read the API docs carefully to get dimensions to line
up. In particular, the conv2d expects that one dimension is the number of input
channels, and another is the number of output channels, so if it's 32 and 64,
then the input was perhaps a (x,x,32)-dimensional tensor and the output would be
(x,x,64)-dimensional, assuming that the padding is set to 'SAME' to avoid
confusing resizes. The x in MNIST is 28, of course.

Name scopes: use a `with tf.name_scope(...):` code block so that the text within
the name_scope method is appended as a _prefix_ to variable names. See tut03 for
examples. It's useful in case we want to keep track by name and have layers that
would otherwise be the same. For instance, `bias` could be the name of bias
weights, but those are typically in every layer of the net, so there's potential
for name clashes. Note: I can do a name_scope block, start a new one (_not_
nested in it but a new one at the same indentation level) and can still refer to
the previous variables generated. Nice!

Recurrent neural nets: a lot of this is just copying their code as usual.
However, I had to put in some manual fixes as shown by others. See:
https://github.com/tensorflow/models/pull/824/commits/a6082b5463840b0840dcf545366dca85966c5ae1
EDIT: never mind, I gave up, I upgraded to Tensorflow v1.0. Now things seem to
be running ... but wow, this is SLOW. Even for the small model. =(

Custom format data: TODO

Tensorboard: TODO

tf.contrib.learn: TODO
